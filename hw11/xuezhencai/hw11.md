# Homework 11
## 1.Einsum Notation

| number | mathematic               | Einsum Notation        | Julia                          |
| ---- | ------------------- | ---------------------- | --------------------------------- |
| 1    | $A v$               | `ij,j -> i`            | `@einsum y[i] := A[i,j]*v[j]`     |
| 2    | $\mathrm{Tr}(A)$    | `ii ->`                | `@einsum tr[] := A[i,i]`          |
| 3    | $A^T$               | `ij -> ji`             | `@einsum AT[j,i] := A[i,j]`       |
| 4    | Row sum             | `ij -> j`              | `@einsum rowsum[j] := A[i,j]`     |
| 5    | Multiply 5 matrices | `ij,jk,kl,lm,mn -> in` | `@einsum M[i,n] := A1[i,j]*...`   |
| 6    | Hadamard product    | `ij,ij -> ij`          | `@einsum H[i,j] := A[i,j]*B[i,j]` |

## 2.Hidden Markov Model
```julia
using LinearAlgebra

#task2

function baum_welch(obs::Vector{Int}, num_states::Int; max_iter::Int=100, tol::Float64=1e-6)
    N = length(obs)
    M = maximum(obs) + 1  # 观测符号数量
    A = rand(num_states, num_states)
    B = rand(num_states, M)
    π = rand(num_states)

    A ./= sum(A, dims=2)
    B ./= sum(B, dims=2)
    π ./= sum(π)

    prev_ll = -Inf

    for iter in 1:max_iter
        # Forward
        α = zeros(N, num_states)
        α[1, :] .= π .* B[:, obs[1]+1]
        for t in 2:N
            for j in 1:num_states
                α[t, j] = sum(α[t-1, i] * A[i, j] for i in 1:num_states) * B[j, obs[t]+1]
            end
        end

        # Backward
        β = ones(N, num_states)
        for t in (N-1):-1:1
            for i in 1:num_states
                β[t, i] = sum(A[i, j] * B[j, obs[t+1]+1] * β[t+1, j] for j in 1:num_states)
            end
        end

        # Gamma and Xi
        γ = α .* β
        γ ./= sum(γ, dims=2)

        ξ = zeros(num_states, num_states, N-1)
        for t in 1:N-1
            denom = sum(α[t, i] * A[i, j] * B[j, obs[t+1]+1] * β[t+1, j] for i in 1:num_states, j in 1:num_states)
            for i in 1:num_states, j in 1:num_states
                ξ[i, j, t] = α[t, i] * A[i, j] * B[j, obs[t+1]+1] * β[t+1, j] / denom
            end
        end

        # Update π
        π .= γ[1, :]

        # Update A
        for i in 1:num_states
            for j in 1:num_states
                numer = sum(ξ[i, j, t] for t in 1:N-1)
                denom = sum(γ[t, i] for t in 1:N-1)
                A[i, j] = numer / denom
            end
        end

        # Update B
        for j in 1:num_states
            denom = sum(γ[t, j] for t in 1:N)
            for k in 0:M-1
                numer = sum(γ[t, j] for t in 1:N if obs[t] == k)
                B[j, k+1] = numer / denom
            end
        end

        # Normalize
        A ./= sum(A, dims=2)
        B ./= sum(B, dims=2)

        # Check log-likelihood for convergence
        ll = log(sum(α[end, :]))
        println("Iter $iter | Log-Likelihood: ", round(ll, digits=6))
        if abs(ll - prev_ll) < tol
            println("✅ Converged at iteration $iter")
            break
        end
        prev_ll = ll
    end

    return A, B
end

# --------------------
# 测试代码
# --------------------

obs = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
num_states = 2

A, B = baum_welch(obs, num_states)

println("\n✅ Learned Transition Matrix A:")
display(A)

println("\n✅ Learned Emission Matrix B:")
display(B)
```
```
Iter 1 | Log-Likelihood: -7.001228
Iter 2 | Log-Likelihood: -6.930932
Iter 3 | Log-Likelihood: -6.926444
Iter 4 | Log-Likelihood: -6.92546
Iter 5 | Log-Likelihood: -6.924563
Iter 6 | Log-Likelihood: -6.923555
Iter 7 | Log-Likelihood: -6.922409
Iter 8 | Log-Likelihood: -6.921109
Iter 9 | Log-Likelihood: -6.919639
Iter 10 | Log-Likelihood: -6.91798
Iter 11 | Log-Likelihood: -6.916113
Iter 12 | Log-Likelihood: -6.914018
Iter 13 | Log-Likelihood: -6.911675
Iter 14 | Log-Likelihood: -6.909063
Iter 15 | Log-Likelihood: -6.906159
Iter 16 | Log-Likelihood: -6.90294
Iter 17 | Log-Likelihood: -6.899378
Iter 18 | Log-Likelihood: -6.895443
Iter 19 | Log-Likelihood: -6.891096
Iter 20 | Log-Likelihood: -6.886286
Iter 21 | Log-Likelihood: -6.880948
Iter 22 | Log-Likelihood: -6.874987
Iter 23 | Log-Likelihood: -6.868272
Iter 24 | Log-Likelihood: -6.860615
Iter 25 | Log-Likelihood: -6.851741
Iter 26 | Log-Likelihood: -6.841241
Iter 27 | Log-Likelihood: -6.828499
Iter 28 | Log-Likelihood: -6.812553
Iter 29 | Log-Likelihood: -6.79185
Iter 30 | Log-Likelihood: -6.763793
Iter 31 | Log-Likelihood: -6.723856
Iter 32 | Log-Likelihood: -6.663941
Iter 33 | Log-Likelihood: -6.569836
Iter 34 | Log-Likelihood: -6.420676
Iter 35 | Log-Likelihood: -6.204392
Iter 36 | Log-Likelihood: -5.948589
Iter 37 | Log-Likelihood: -5.638974
Iter 38 | Log-Likelihood: -5.034651
Iter 39 | Log-Likelihood: -3.574304
Iter 40 | Log-Likelihood: -1.140924
Iter 41 | Log-Likelihood: -0.032701
Iter 42 | Log-Likelihood: -1.0e-6
Iter 43 | Log-Likelihood: 0.0
✅ Converged at iteration 43
([4.1585243043326136e-63 1.0; 1.0 2.5104199020254666e-67], [1.6634100474115932e-63 1.0; 1.0 2.510744547186035e-67])


✅ Learned Transition Matrix A:

2×2 Matrix{Float64}:
 4.15852e-63  1.0
 1.0          2.51042e-67


✅ Learned Emission Matrix B:

2×2 Matrix{Float64}:
 1.66341e-63  1.0
 1.0          2.51074e-67
```